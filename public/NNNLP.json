[
  {
    "id": 1,
    "question": "מה מאפיין מרכזי של רשת Feedforward בסיסית בלמידה עמוקה?",
    "options": [
      {
        "id": "a2",
        "text": "הרשת מתבססת על לולאות זמן פנימיות בלבד"
      },
      {
        "id": "a3",
        "text": "הרשת מייצרת רק הטמעות ללא אימון"
      },
      {
        "id": "a4",
        "text": "הרשת מבצעת טרנספורמציות ליניאריות ואז לא־ליניאריות רציפות"
      },
      {
        "id": "a1",
        "text": "הרשת פועלת רק עם שכבות קונבולוציה קבועות"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "00:07:31–00:10:07",
      "quote": "הוסבר מבנה רשתות Feedforward: טרנספורמציות לינאריות ולא־לינאריות ויעד אופטימיזציה."
    }
  },
  {
    "id": 2,
    "question": "מהו רצף הצעדים הכללי באימון רשת כמתואר בשיעור?",
    "options": [
      {
        "id": "a3",
        "text": "חיזוי → Beam Search → דגימה → Fine‑tuning"
      },
      {
        "id": "a4",
        "text": "Forward → חישוב Loss → Backpropagation → עדכון משקולות"
      },
      {
        "id": "a1",
        "text": "Backpropagation → יצירת דאטה → Tokenization → Loss"
      },
      {
        "id": "a2",
        "text": "Pooling → קונבולוציה → נורמליזציה → Decoding"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "00:11:02–00:11:21",
      "quote": "אימון מתבצע ב‑Forward, חישוב הפסד, ואז Backpropagation לתיקון משקולות."
    }
  },
  {
    "id": 3,
    "question": "כיצד מיוצגות מילים במודל לפני ואחרי שכבת ההטמעות (Embedding)?",
    "options": [
      {
        "id": "a4",
        "text": "לפני כן One‑Hot דליל, אחרי כן וקטור צפוף מה‑Embedding"
      },
      {
        "id": "a1",
        "text": "לפני כן תמונת פיקסלים, אחרי כן מטריצת קונבולוציה"
      },
      {
        "id": "a2",
        "text": "לפני כן BPE, אחרי כן מפת חום דו‑ממדית"
      },
      {
        "id": "a3",
        "text": "לפני כן מספר שלם, אחרי כן סקלר יחיד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "00:11:23–00:13:40",
      "quote": "קידוד One‑Hot ושימוש במטריצת Embedding לוקטורים צפופים."
    }
  },
  {
    "id": 4,
    "question": "למה משתמשים בלמידת העברה (Transfer Learning) ב‑NLP?",
    "options": [
      {
        "id": "a1",
        "text": "כדי להחליף את כל שכבות המודל בכל איטרציה"
      },
      {
        "id": "a2",
        "text": "כדי להימנע לחלוטין מ‑Fine‑tuning בכל מצב"
      },
      {
        "id": "a3",
        "text": "כדי לאמן רק על זוגות משפטים מקבילים"
      },
      {
        "id": "a4",
        "text": "כדי לנצל ידע ממודל קדם‑מאומן ולהתאים אותו לדאטה קטן וספציפי"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "00:13:44–00:15:32",
      "quote": "ניצול מודלים גדולים קדם‑מאומנים והתאמתם לדומיין מצומצם."
    }
  },
  {
    "id": 5,
    "question": "מהי המוטיבציה המרכזית ל‑Word2Vec כפי שהוסברה?",
    "options": [
      {
        "id": "a2",
        "text": "להגדיל בכוונה את עלות החישוב של סופטמקס"
      },
      {
        "id": "a3",
        "text": "לוותר על שימוש בהטמעות כלל"
      },
      {
        "id": "a4",
        "text": "ללמוד ייצוגי מילים יעילים ללא חישוב Softmax על כל המילון"
      },
      {
        "id": "a1",
        "text": "להחליף את כל המודל ב‑CNN בלבד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "00:15:34–00:18:22",
      "quote": "עלות Softmax גבוהה במודלי שפה; Word2Vec מציע דרך יעילה ללמידת אמבדינגים."
    }
  },
  {
    "id": 6,
    "question": "מה ההבדל בין CBOW ל‑Skip‑gram ב‑Word2Vec?",
    "options": [
      {
        "id": "a3",
        "text": "CBOW הוא מודל קונבולוציוני; Skip‑gram רקורנטי"
      },
      {
        "id": "a4",
        "text": "CBOW חוזה מילה מרכזית מהקשר; Skip‑gram חוזה הקשר ממילה מרכזית"
      },
      {
        "id": "a1",
        "text": "CBOW בוחר חלון משתנה; Skip‑gram חלון קבוע בלבד"
      },
      {
        "id": "a2",
        "text": "CBOW משתמש ב‑Softmax דו‑שלבי; Skip‑gram ללא לוס"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "00:27:13–00:31:34",
      "quote": "הוגדרו שתי הארכיטקטורות: חיזוי מילה מהקשר מול חיזוי הקשר מהמילה."
    }
  },
  {
    "id": 7,
    "question": "מהי האינטואיציה של מודל המטרה ב‑Word2Vec כפי שהוצגה?",
    "options": [
      {
        "id": "a4",
        "text": "להעלות סקור לזוגות מילים קשורות ולהורידו ללא קשורות עם מרג'ין"
      },
      {
        "id": "a1",
        "text": "להחליף את הסקור בהגרלה אקראית של מילים"
      },
      {
        "id": "a2",
        "text": "לעדכן רק את מילת המטרה ולעולם לא את ההקשר"
      },
      {
        "id": "a3",
        "text": "לחשב לוס רק על זוגות זהים לחלוטין"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "00:37:40–00:48:47",
      "quote": "מקסום סקור לחיוביים ומיזעור לשליליים, כולל הוספת מרווח הפרדה."
    }
  },
  {
    "id": 8,
    "question": "מדוע ל‑Word2Vec יש שני וקטורים לכל מילה (U ו‑V)?",
    "options": [
      {
        "id": "a1",
        "text": "כדי להכפיל את גודל המודל ללא תועלת"
      },
      {
        "id": "a2",
        "text": "כדי לאפשר רק חיזוי דו‑כיווני בזמן אמת"
      },
      {
        "id": "a3",
        "text": "כדי לשמור היסטוריה של אפוקים בלבד"
      },
      {
        "id": "a4",
        "text": "אחד כמילה מרכזית ואחד כמילת הקשר כדי לייצג תפקידים שונים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "00:52:00–00:59:00",
      "quote": "ה‑Objective בסקיפ‑גרם משתמש בייצוגים נפרדים למרכז ולקונטקסט."
    }
  },
  {
    "id": 9,
    "question": "כיצד נוצרים זוגות האימון ב‑Skip‑gram?",
    "options": [
      {
        "id": "a2",
        "text": "נבחרת רק המילה הראשונה של המסמך תמיד"
      },
      {
        "id": "a3",
        "text": "נוצרים זוגות רק ממילים שאינן שכנות"
      },
      {
        "id": "a4",
        "text": "חולון זז מייצר (מילה מרכזית, מילת קונטקסט) לכל שכנים בטווח"
      },
      {
        "id": "a1",
        "text": "כל המשפט נלקח כזוג יחיד לכל איטרציה"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "01:22:50–01:29:10",
      "quote": "מעבר בחלונות ומיצוי זוגות מרכז‑קונטקסט לאימון."
    }
  },
  {
    "id": 10,
    "question": "איזו תופעה מעניינת הודגמה בהטמעות Word2Vec?",
    "options": [
      {
        "id": "a3",
        "text": "איבוד מוחלט של יחסים תחביריים"
      },
      {
        "id": "a4",
        "text": "אלגברה סמנטית על וקטורי מילים (כמו מלך − גבר + אישה ≈ מלכה)"
      },
      {
        "id": "a1",
        "text": "מיפוי מילים לתמונות ישירות ללא מודל"
      },
      {
        "id": "a2",
        "text": "יכולת תרגום מושלמת ללא דאטה"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "01:22:50–01:29:10",
      "quote": "האמבדינגים לומדים יחסים סמנטיים המאפשרים חיבור/חיסור וקטורים משמעותי."
    }
  },
  {
    "id": 11,
    "question": "למה משמש Subsampling ב‑Word2Vec?",
    "options": [
      {
        "id": "a4",
        "text": "להפחית הופעות של מילים שכיחות מאוד כדי למקד בלמידה משמעותית"
      },
      {
        "id": "a1",
        "text": "להגדיל משקל למילות עצירה בכל משפט"
      },
      {
        "id": "a2",
        "text": "להחליף את Softmax באקראיות מלאה"
      },
      {
        "id": "a3",
        "text": "להוסיף עוד שכבות קונבולוציה"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "01:42:05–01:56:44",
      "quote": "דגימת‑משנה של מילים תדירות מאזנת התפלגויות ומאיצה אימון."
    }
  },
  {
    "id": 12,
    "question": "מה היתרון של Negative Sampling?",
    "options": [
      {
        "id": "a1",
        "text": "חישוב גרדיאנטים ללא לוס כלל"
      },
      {
        "id": "a2",
        "text": "אימון רק על משפטים קצרים בלבד"
      },
      {
        "id": "a3",
        "text": "דרישה לזיכרון אינסופי למילון"
      },
      {
        "id": "a4",
        "text": "עדכון מספר קטן של מילים שליליות במקום כל המילון בכל צעד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "01:56:51–02:11:00",
      "quote": "דגימה שלילית מורידה עלות סופטמקס ומאיצה את העדכונים."
    }
  },
  {
    "id": 13,
    "question": "מה החידוש המרכזי של GloVe לעומת Word2Vec?",
    "options": [
      {
        "id": "a2",
        "text": "שימוש רק במשפט הראשון בכל מסמך"
      },
      {
        "id": "a3",
        "text": "לימוד ללא מטריצות כלל"
      },
      {
        "id": "a4",
        "text": "שילוב סטטיסטיקה גלובלית של הופעות משותפות במטריצת Co‑occurrence"
      },
      {
        "id": "a1",
        "text": "ביטול מוחלט של מידע קונטקסטואלי"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "02:11:40–02:20:07",
      "quote": "GloVe משתמש ביחסי הופעה גלובליים לצד מידע לוקלי לייצוגים עשירים."
    }
  },
  {
    "id": 14,
    "question": "לאיזה גודל מכוון הלוס של GloVe ביחס להכפלה הסקלרית בין וקטורים?",
    "options": [
      {
        "id": "a3",
        "text": "למרחק האוקלידי בלבד"
      },
      {
        "id": "a4",
        "text": "ללוגריתם של תדירות ההופעה המשותפת"
      },
      {
        "id": "a1",
        "text": "לריבוע של אורך המשפט המקורי"
      },
      {
        "id": "a2",
        "text": "למספר השכבות ב‑CNN"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 1 — NN-NLP 05-05-2024",
      "time": "02:20:11–02:27:10",
      "quote": "האינטואיציה: inner‑product ≈ log של סטטיסטיקת קו‑הופעה."
    }
  },
  {
    "id": 15,
    "question": "מה תפקיד פונקציית המשקל ב‑GloVe עבור מילים תדירות מאוד?",
    "options": [
      {
        "id": "a4",
        "text": "להגביל את תרומתן כך שלא ישתלטו על האופטימיזציה"
      },
      {
        "id": "a1",
        "text": "להכפיל את משקלן בכל הופעה מחדש"
      },
      {
        "id": "a2",
        "text": "להעלים לחלוטין מילים נדירות מהאימון"
      },
      {
        "id": "a3",
        "text": "להפוך את הלוס ללא דיפרנציאבילי"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 2 — NN-NLP 12-05-2024",
      "time": "00:39:19–00:47:59",
      "quote": "פונקציית משקל מרסנת מילים נפוצות מ‑over‑fitting."
    }
  },
  {
    "id": 16,
    "question": "כיצד מטפלים בביטויים מרובי מילים כמו 'ice cream'?",
    "options": [
      {
        "id": "a1",
        "text": "מפצלים כל ביטוי לאותיות בודדות"
      },
      {
        "id": "a2",
        "text": "מוחקים את כל המופעים מהקורפוס"
      },
      {
        "id": "a3",
        "text": "מחליפים בביטוי אקראי דומה"
      },
      {
        "id": "a4",
        "text": "מזהים סטטיסטית N‑grams ומקצים להם טוקן אחד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 2 — NN-NLP 12-05-2024",
      "time": "00:48:00–00:59:58",
      "quote": "דוגמאות לשימוש ב‑N‑gram סטטיסטי כדי לאחד ביטויי מפתח."
    }
  },
  {
    "id": 17,
    "question": "כיצד מכלילים את רעיון ההטמעות מעבר למילים?",
    "options": [
      {
        "id": "a2",
        "text": "מייצגים רק משפטים כטבלאות"
      },
      {
        "id": "a3",
        "text": "מחליפים כל טקסט בתמונה"
      },
      {
        "id": "a4",
        "text": "ממפים קטגוריות כלליות (למשל משתמשים/צבעים) לוקטורים לומדים"
      },
      {
        "id": "a1",
        "text": "ממפים רק מספרים ממשיים לריבועים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 2 — NN-NLP 12-05-2024",
      "time": "01:00:00–01:12:44",
      "quote": "הטמעות לתכונות קטגוריאליות משפרות ייצוג יחסים במערכות לומדות."
    }
  },
  {
    "id": 18,
    "question": "מה מגבלה עיקרית של מודל Bag‑of‑Words?",
    "options": [
      {
        "id": "a3",
        "text": "דרישה לקידוד תמונות בלבד"
      },
      {
        "id": "a4",
        "text": "אובדן מידע על סדר מילים במשפטים"
      },
      {
        "id": "a1",
        "text": "אי‑יכולת לאמן כלל על טקסטים ארוכים"
      },
      {
        "id": "a2",
        "text": "חוסר תמיכה במילונים גדולים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 2 — NN-NLP 12-05-2024",
      "time": "01:17:59–01:23:55",
      "quote": "BoW יעיל לסיווגים מסוימים אך מאבד סדר וקשרים רציפים."
    }
  },
  {
    "id": 19,
    "question": "למה CNNs שימושיות ב‑NLP לפי ההסבר?",
    "options": [
      {
        "id": "a4",
        "text": "לכידת מאפיינים מקומיים (n‑grams) באופן יעיל ומקבילי"
      },
      {
        "id": "a1",
        "text": "חיזוי טוקן הבא על בסיס זיכרון אין‑סופי"
      },
      {
        "id": "a2",
        "text": "תמיכה מובנית בבי‑דירקטיביות גלובלית"
      },
      {
        "id": "a3",
        "text": "ביטול הצורך בהטמעות כלל"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 2 — NN-NLP 12-05-2024",
      "time": "01:44:24–01:52:15",
      "quote": "קונבולוציות לומדות תבניות לוקליות בדומה ל‑n‑grams."
    }
  },
  {
    "id": 20,
    "question": "מה משמעות שימוש בפילטרים 1D בגדלים שונים (2,3 על K) ב‑CNN לטקסט?",
    "options": [
      {
        "id": "a1",
        "text": "כל פילטר מכפיל את אורך המשפט פעמיים"
      },
      {
        "id": "a2",
        "text": "הפילטרים משמשים רק לנורמליזציה"
      },
      {
        "id": "a3",
        "text": "הם נדרשים רק ל‑ImageNet"
      },
      {
        "id": "a4",
        "text": "כל פילטר לוכד n‑gram באורך אחר ומוסיף ערוץ פיצ'רים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 2 — NN-NLP 12-05-2024",
      "time": "02:16:22–02:24:58",
      "quote": "פילטרים שונים מזהים n‑grams באורכים שונים."
    }
  },
  {
    "id": 21,
    "question": "מדוע Max Pooling נפוץ בסיווג משפטים עם CNN?",
    "options": [
      {
        "id": "a2",
        "text": "מונע Backpropagation בכל השכבות"
      },
      {
        "id": "a3",
        "text": "מחליף את הצורך ב‑Fully Connected"
      },
      {
        "id": "a4",
        "text": "מייצר ייצוג קבוע באורכו ומאפשר פרשנות של 'מה הפעיל את המסלול'"
      },
      {
        "id": "a1",
        "text": "מוחק את כל הפיצ'רים החלשים לחלוטין"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 2 — NN-NLP 12-05-2024",
      "time": "02:24:59–02:31:33",
      "quote": "Pooling מסכם את החזקים ומאפשר מעבר ל‑FC ברוחב קבוע."
    }
  },
  {
    "id": 22,
    "question": "מהו הצורך העיקרי שה‑RNN פותר לעומת BoW ו‑CNN?",
    "options": [
      {
        "id": "a3",
        "text": "החלפת ההטמעות במספרים אקראיים"
      },
      {
        "id": "a4",
        "text": "למידת סדר גלובלי ברצפים באורך משתנה"
      },
      {
        "id": "a1",
        "text": "שימוש רק במידע מהטוקן האחרון"
      },
      {
        "id": "a2",
        "text": "קונבולוציה דו‑ממדית על תמונות בלבד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 3 — NN-NLP 30-03-2025",
      "time": "00:08:56–00:10:05",
      "quote": "RNN מקודד היסטוריה לאורך הרצף ומטפל באורכים משתנים."
    }
  },
  {
    "id": 23,
    "question": "מהם המרכיבים הבסיסיים של RNN ונגזרותיו?",
    "options": [
      {
        "id": "a4",
        "text": "קלט x_t, מצב נסתר h_t, ופלט y_t עם משקולות משותפות בזמן"
      },
      {
        "id": "a1",
        "text": "קלט תמונה, מפה קונבולוציונית, ומסכה קבועה"
      },
      {
        "id": "a2",
        "text": "זיכרון קבוע ללא עדכון, ופלט יחיד בלבד"
      },
      {
        "id": "a3",
        "text": "שכבות Decoding בלבד ללא קידוד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 3 — NN-NLP 30-03-2025",
      "time": "00:09:29–00:12:57",
      "quote": "הוצג מבנה RNN פרוש בזמן, משקולות זהות בכל צעד."
    }
  },
  {
    "id": 24,
    "question": "איזו התאמה נכונה בין קונפיגורציה למשימה?",
    "options": [
      {
        "id": "a1",
        "text": "Many‑to‑One מתאים לתרגום; Many‑to‑Many לסיכום"
      },
      {
        "id": "a2",
        "text": "Many‑to‑Many מתאים לרגרסיה; Many‑to‑One לתגמול"
      },
      {
        "id": "a3",
        "text": "Many‑to‑Many רק לשמע; Many‑to‑One רק לתמונה"
      },
      {
        "id": "a4",
        "text": "Many‑to‑Many מתאים ל‑NER; Many‑to‑One מתאים לסיווג משפטים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 3 — NN-NLP 30-03-2025",
      "time": "00:14:48–00:16:48",
      "quote": "דוגמת NER עם BIO לעומת סיווג ברמת משפט."
    }
  },
  {
    "id": 25,
    "question": "מה עושה Backpropagation Through Time (BPTT) ב‑RNN?",
    "options": [
      {
        "id": "a2",
        "text": "מדלג על חישוב שגיאה בכל צעד"
      },
      {
        "id": "a3",
        "text": "מעביר משקולות חדשות לכל טוקן"
      },
      {
        "id": "a4",
        "text": "מחשב גרדיאנטים לאורך ציר הזמן עם כלל השרשרת"
      },
      {
        "id": "a1",
        "text": "מחליף את ההטמעות בקונבולוציות"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 3 — NN-NLP 30-03-2025",
      "time": "00:41:13–00:49:00",
      "quote": "BPTT מפזר את השגיאה אחורה בזמן לעדכון משקולות."
    }
  },
  {
    "id": 26,
    "question": "מהי בעיית Vanishing Gradient ב‑RNN?",
    "options": [
      {
        "id": "a3",
        "text": "המודל תמיד זוכר את כל ההיסטוריה"
      },
      {
        "id": "a4",
        "text": "גרדיאנטים קטנים מתאפסים לאורך צעדי זמן רבים וקשה ללמוד תלות רחוקה"
      },
      {
        "id": "a1",
        "text": "הגרדיאנטים גדלים עד אינסוף בכל צעד"
      },
      {
        "id": "a2",
        "text": "אין אפשרות לבצע חישוב מקבילי על באצ'ים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 3 — NN-NLP 30-03-2025",
      "time": "01:08:24–01:09:32",
      "quote": "מכפלות נגזרות קטנות גורמות לחיוורון הגרדיאנט לאורך זמן."
    }
  },
  {
    "id": 27,
    "question": "כיצד מטפלים ב‑Exploding Gradients לפי השיעור?",
    "options": [
      {
        "id": "a4",
        "text": "Gradient Clipping להגבלת גודל הגרדיאנט מעל סף"
      },
      {
        "id": "a1",
        "text": "הוספת שכבות ללא אקטיבציה"
      },
      {
        "id": "a2",
        "text": "ביטול Backpropagation לחלוטין"
      },
      {
        "id": "a3",
        "text": "הגדלת קצב הלמידה ללא גבול"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 3 — NN-NLP 30-03-2025",
      "time": "01:24:02–01:26:22",
      "quote": "קיצוץ גרדיאנטים מייצב את האימון כשערכיהם מתפוצצים."
    }
  },
  {
    "id": 28,
    "question": "מה תפקיד מצב התא (Cell State) ב‑LSTM?",
    "options": [
      {
        "id": "a1",
        "text": "להחליף את hidden state לחלוטין"
      },
      {
        "id": "a2",
        "text": "למדל קונבולוציות על תמונות"
      },
      {
        "id": "a3",
        "text": "למנוע למידה באמצעות הקפאה קבועה"
      },
      {
        "id": "a4",
        "text": "לאחסן מידע לטווח ארוך ולעבור עם שערים בין צעדים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 4 — NN-NLP 20-04-2025",
      "time": "01:33:17–01:37:51",
      "quote": "LSTM מוסיף Cell State ושערים לשימור ושכחה מבוקרת."
    }
  },
  {
    "id": 29,
    "question": "איזה שער ב‑LSTM שולט בכמה מידע חדש נכנס לתא?",
    "options": [
      {
        "id": "a2",
        "text": "Output Gate קובע את שכבת האיבוד"
      },
      {
        "id": "a3",
        "text": "Pooling Gate מבצע ריכוך"
      },
      {
        "id": "a4",
        "text": "Input Gate קובע את כתיבת המידע החדש"
      },
      {
        "id": "a1",
        "text": "Forget Gate קובע את קריאת הפלט"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 4 — NN-NLP 20-04-2025",
      "time": "01:41:16–01:46:51",
      "quote": "שערי Forget/Input/Output מנהלים זרימת מידע בתא."
    }
  },
  {
    "id": 30,
    "question": "במה GRU שונה מ‑LSTM?",
    "options": [
      {
        "id": "a3",
        "text": "אינה תומכת כלל ב‑Backpropagation"
      },
      {
        "id": "a4",
        "text": "פחות שערים, ללא Cell State נפרד ולכן חישובית קלה יותר"
      },
      {
        "id": "a1",
        "text": "יותר שערים ומצב תא כפול"
      },
      {
        "id": "a2",
        "text": "נדרשת רק למודלי חזון"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 4 — NN-NLP 20-04-2025",
      "time": "01:59:26–02:03:09",
      "quote": "GRU כוללת שער עדכון ואיפוס, מבנה קומפקטי לעומת LSTM."
    }
  },
  {
    "id": 31,
    "question": "למה משמש RNN דו‑כיווני (BiRNN)?",
    "options": [
      {
        "id": "a4",
        "text": "לנצל קונטקסט מהעבר ומהעתיד למשימות הבנה"
      },
      {
        "id": "a1",
        "text": "לחיזוי המילה הבאה בזמן אמת בלבד"
      },
      {
        "id": "a2",
        "text": "להחלפת אמבדינגים סטטיים"
      },
      {
        "id": "a3",
        "text": "לצמצום המודל לשכבה אחת"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 4 — NN-NLP 20-04-2025",
      "time": "02:03:14–02:05:56",
      "quote": "BiRNN מאמן שני כיוונים ומחברם לקונטקסט מלא."
    }
  },
  {
    "id": 32,
    "question": "מה משמעות תגי B‑I‑O ב‑NER?",
    "options": [
      {
        "id": "a1",
        "text": "בסיס, אינדוקציה, אופטימיזציה"
      },
      {
        "id": "a2",
        "text": "ביטול, יישור, אופרטור"
      },
      {
        "id": "a3",
        "text": "בלוק, איטרציה, אופק"
      },
      {
        "id": "a4",
        "text": "תחילת ישות, בתוך ישות, ואחר (לא ישות)"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 4 — NN-NLP 20-04-2025",
      "time": "00:16:48–00:17:59",
      "quote": "דוגמה לתיוג ישויות מרובות מילים באמצעות BIO."
    }
  },
  {
    "id": 33,
    "question": "מהו רעיון ה‑Sequence‑to‑Sequence (Seq2Seq)?",
    "options": [
      {
        "id": "a2",
        "text": "RNN מחליף את ההטמעות בסכימה"
      },
      {
        "id": "a3",
        "text": "Transformer פועל ללא קידוד כלל"
      },
      {
        "id": "a4",
        "text": "Encoder מקודד את הקלט ו‑Decoder מייצר פלט באורך משתנה"
      },
      {
        "id": "a1",
        "text": "CNN מייצר תמיד פלט באורך קבוע"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 5 — NN-NLP 27-04-2025",
      "time": "00:43:15–00:47:46",
      "quote": "מודל קידוד‑פענוח מקצה לקצה למשימות כמו תרגום."
    }
  },
  {
    "id": 34,
    "question": "כיצד משלבים 'קונדישן' בגנרציה מותנית לפי השיעור?",
    "options": [
      {
        "id": "a3",
        "text": "באמצעות קידוד תמונה בודדת"
      },
      {
        "id": "a4",
        "text": "באמצעות שרשור וקטור התנאי לייצוג הקלט/פלט"
      },
      {
        "id": "a1",
        "text": "באמצעות מחיקת כל המידע על הסגנון"
      },
      {
        "id": "a2",
        "text": "על ידי שימוש רק בטוקן סיום"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 5 — NN-NLP 27-04-2025",
      "time": "00:01:42–00:05:47",
      "quote": "שרשור (concatenation) של הייצוג מכניס תנאי ל‑Decoder."
    }
  },
  {
    "id": 35,
    "question": "כיצד פורמלית קלאסית מתארים תרגום מכונה לפני NMT?",
    "options": [
      {
        "id": "a4",
        "text": "בפיצול לפי חוק בייס: מודל תרגום × מודל שפה"
      },
      {
        "id": "a1",
        "text": "ביטול מוחלט של מודל השפה"
      },
      {
        "id": "a2",
        "text": "במספר קבוע של חוקים ידניים"
      },
      {
        "id": "a3",
        "text": "ב‑CNN דו‑ממדית בלבד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 5 — NN-NLP 27-04-2025",
      "time": "00:37:57–00:40:07",
      "quote": "הצגה של Translation Model ו‑Language Model ברוח Bayes."
    }
  },
  {
    "id": 36,
    "question": "מה ההבדל העיקרי בין BLEU ל‑ROUGE?",
    "options": [
      {
        "id": "a1",
        "text": "BLEU מודד זמן ריצה; ROUGE מודד זיכרון"
      },
      {
        "id": "a2",
        "text": "BLEU מתאים רק לדיבור; ROUGE רק לראייה"
      },
      {
        "id": "a3",
        "text": "שניהם מודדים זהות סינטקטית בלבד"
      },
      {
        "id": "a4",
        "text": "BLEU מדגיש Precision; ROUGE מדגיש Recall על N‑grams"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 5 — NN-NLP 27-04-2025",
      "time": "00:52:05–00:56:41",
      "quote": "מטריקות חפיפת n‑grams עם דגשי דיוק מול שליפה."
    }
  },
  {
    "id": 37,
    "question": "איזו בעיה פותר מנגנון Attention ב‑Seq2Seq?",
    "options": [
      {
        "id": "a2",
        "text": "צורך בהטמעות מבוססות תמונה"
      },
      {
        "id": "a3",
        "text": "תלות בסופטמקס מרובה שלבים"
      },
      {
        "id": "a4",
        "text": "צוואר בקבוק של דחיסת כל הקלט לוקטור יחיד"
      },
      {
        "id": "a1",
        "text": "חוסר יכולת של רשתות ללמוד בכלל"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 5 — NN-NLP 27-04-2025",
      "time": "01:02:02–01:05:28",
      "quote": "Attention מספק קונטקסט דינמי בהתאם לפלט הנוכחי."
    }
  },
  {
    "id": 38,
    "question": "כיצד נקבעות משקולות ה‑Attention (ה‑'אלפות')?",
    "options": [
      {
        "id": "a3",
        "text": "נקבעות לפי מיקום בלבד"
      },
      {
        "id": "a4",
        "text": "MLP מחשב ציוני דמיון ומנרמל ב‑Softmax לסכום 1"
      },
      {
        "id": "a1",
        "text": "הן קבועות מראש ולא נלמדות"
      },
      {
        "id": "a2",
        "text": "נבחרות באקראי בכל אפוק"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 5 — NN-NLP 27-04-2025",
      "time": "01:10:01–01:18:04",
      "quote": "אלפות מחושבות פונקציונלית ונורמליזציה ב‑Softmax לפרשנות."
    }
  },
  {
    "id": 39,
    "question": "מהו החיסרון החישובי הבולט של Attention?",
    "options": [
      {
        "id": "a4",
        "text": "מורכבות זמן ריבועית באורך הרצף (N×M)"
      },
      {
        "id": "a1",
        "text": "אי‑דיפרנציאביליות של הלוס"
      },
      {
        "id": "a2",
        "text": "זיכרון של O(1) בלבד לכל טוקן"
      },
      {
        "id": "a3",
        "text": "חוסר תמיכה ב‑Backpropagation"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 5 — NN-NLP 27-04-2025",
      "time": "01:28:20–01:29:58",
      "quote": "העלות הריבועית מגבילה חלונות קונטקסט ארוכים."
    }
  },
  {
    "id": 40,
    "question": "מהו מרכיב הליבה שמייחד טרנספורמרים?",
    "options": [
      {
        "id": "a1",
        "text": "קונבולוציות דו‑ממדיות בלבד"
      },
      {
        "id": "a2",
        "text": "לולאות זמן קשיחות בכל שכבה"
      },
      {
        "id": "a3",
        "text": "מיפוי חד‑חד‑ערכי למספרים"
      },
      {
        "id": "a4",
        "text": "Self‑Attention שמאפשר לכל טוקן להתייחס לכל האחרים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 6 — NN-NLP 04-05-2025",
      "time": "00:17:12–00:19:09",
      "quote": "Self‑Attention יוצר תלותים גלובליים ביעילות מקבילית."
    }
  },
  {
    "id": 41,
    "question": "מהם Q, K, V במנגנון Self‑Attention?",
    "options": [
      {
        "id": "a2",
        "text": "וקטורים אקראיים שאינם נלמדים"
      },
      {
        "id": "a3",
        "text": "תמונות קלט בקידוד RGB"
      },
      {
        "id": "a4",
        "text": "וקטורי Query, Key, Value המתקבלים מהכפלה במטריצות משקל"
      },
      {
        "id": "a1",
        "text": "ערכים קבועים שנבחרים ידנית מראש"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 6 — NN-NLP 04-05-2025",
      "time": "00:20:00–00:23:09",
      "quote": "כל קלט מומר לשלושה ייצוגים Q/K/V בעזרת W_Q, W_K, W_V."
    }
  },
  {
    "id": 42,
    "question": "למה משמש Multi‑Head Attention?",
    "options": [
      {
        "id": "a3",
        "text": "לבטל את ה‑Self‑Attention לגמרי"
      },
      {
        "id": "a4",
        "text": "ללמוד בו‑זמנית סוגי יחסים שונים ואז לקנקט"
      },
      {
        "id": "a1",
        "text": "להחליף את ההטמעות בפונקציות סינוס בלבד"
      },
      {
        "id": "a2",
        "text": "לצמצם את מספר הפרמטרים לאפס"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 6 — NN-NLP 04-05-2025",
      "time": "00:36:09–00:43:56",
      "quote": "ראשים מרובים לוכדים יחסים מגוונים ומעשירים ייצוג."
    }
  },
  {
    "id": 43,
    "question": "מדוע מוסיפים Positional Encoding לטרנספורמר?",
    "options": [
      {
        "id": "a4",
        "text": "כי Self‑Attention לבדו אינו תלוי סדר ויש צורך במידע מיקום מפורש"
      },
      {
        "id": "a1",
        "text": "כדי להגדיל את מספר הפרמטרים ללא צורך"
      },
      {
        "id": "a2",
        "text": "כדי למנוע שימוש ב‑Softmax"
      },
      {
        "id": "a3",
        "text": "כדי לאפשר רק משפטים קצרים"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 6 — NN-NLP 04-05-2025",
      "time": "00:44:44–00:47:26",
      "quote": "קידוד מיקום בסינוס/קוסינוס מעניק מודעות לסדר."
    }
  },
  {
    "id": 44,
    "question": "מה כולל בלוק האנקודר הבסיסי בטרנספורמר?",
    "options": [
      {
        "id": "a1",
        "text": "Pooling → Convolution → Dropout → Upsample"
      },
      {
        "id": "a2",
        "text": "RNN דו‑כיווני → Softmax → Beam"
      },
      {
        "id": "a3",
        "text": "Decoder‑Only → Mask → Sampling"
      },
      {
        "id": "a4",
        "text": "Self‑Attention → Add&Norm → Feed‑Forward → Add&Norm"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 6 — NN-NLP 04-05-2025",
      "time": "01:22:29–01:33:33",
      "quote": "רצף שכבות עם חיבורים ישירים ונורמליזציה בין שלבים."
    }
  },
  {
    "id": 45,
    "question": "מה תפקיד Masked Self‑Attention בדקודר?",
    "options": [
      {
        "id": "a2",
        "text": "לשנות את קידוד המיקום לסינוסידלים"
      },
      {
        "id": "a3",
        "text": "להחליף את Softmax בלולאות"
      },
      {
        "id": "a4",
        "text": "למנוע הצצה לטוקנים עתידיים בזמן יצירה אוטורגרסיבית"
      },
      {
        "id": "a1",
        "text": "לאפשר שימוש ב‑Keys עתידיים בכל מצב"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 6 — NN-NLP 04-05-2025",
      "time": "01:33:57–01:42:25",
      "quote": "מסכה משאירה תלותים רק בעבר כדי לשמור סיבתיות."
    }
  },
  {
    "id": 46,
    "question": "לאילו משימות מתאימים מודלים מבוססי Encoder בלבד (כגון BERT)?",
    "options": [
      {
        "id": "a3",
        "text": "חיזוי פיקסלים ב‑RGB"
      },
      {
        "id": "a4",
        "text": "משימות NLU כמו סיווג, זיהוי ישויות ו‑Extractive QA"
      },
      {
        "id": "a1",
        "text": "גנרציה אוטורגרסיבית ארוכה בלבד"
      },
      {
        "id": "a2",
        "text": "סינתזת תמונה בזמן אמת"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 7 — NN-NLP 11-05-2025",
      "time": "00:13:34–00:20:07",
      "quote": "מודלי Encoder דו‑כיווניים להבנת טקסט."
    }
  },
  {
    "id": 47,
    "question": "מה הרעיון של Byte‑Pair Encoding (BPE) בטוקניזציה?",
    "options": [
      {
        "id": "a4",
        "text": "מיזוג חוזר של זוגות תווים שכיחים ליצירת תת‑מילים"
      },
      {
        "id": "a1",
        "text": "פירוק כל מילה לתו בודד תמיד"
      },
      {
        "id": "a2",
        "text": "המרת טקסט לתדרי סינוס"
      },
      {
        "id": "a3",
        "text": "שימוש רק במילון קבוע מראש"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 7 — NN-NLP 11-05-2025",
      "time": "00:27:26–00:54:16",
      "quote": "BPE מקטין אוצר מילים ומטפל במילים נדירות ושגיאות כתיב."
    }
  },
  {
    "id": 48,
    "question": "כיצד אומן GPT‑1 לפי ההרצאה?",
    "options": [
      {
        "id": "a1",
        "text": "אימון רק על זוגות תרגום מקביליים"
      },
      {
        "id": "a2",
        "text": "ללא Pre‑training וללא Fine‑tuning"
      },
      {
        "id": "a3",
        "text": "רק Masked Language Modeling דו‑כיווני"
      },
      {
        "id": "a4",
        "text": "Pre‑training בלתי מפוקח על חיזוי מילה הבאה ואז Fine‑tuning מפוקח"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 7 — NN-NLP 11-05-2025",
      "time": "00:54:16–01:00:29",
      "quote": "שילוב pre‑training + fine‑tuning למנוע שכחה קטסטרופלית."
    }
  },
  {
    "id": 49,
    "question": "כיצד הודגמה יכולת Zero‑Shot ב‑GPT‑1?",
    "options": [
      {
        "id": "a2",
        "text": "באמצעות קידוד תמונה בודדת"
      },
      {
        "id": "a3",
        "text": "על‑ידי שימוש רק ב‑ROUGE"
      },
      {
        "id": "a4",
        "text": "על‑ידי מסיכת פלט לשמות מחלקות ('Positive'/'Negative') ללא אימון מפורש"
      },
      {
        "id": "a1",
        "text": "באמצעות החלפת כל שכבות המודל"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 7 — NN-NLP 11-05-2025",
      "time": "01:06:22–01:13:58",
      "quote": "טריק של מיסוך הלוגיטים איפשר ביצוע משימה שלא אומן עליה."
    }
  },
  {
    "id": 50,
    "question": "מהן שתי משימות האימון המקדימות של BERT?",
    "options": [
      {
        "id": "a3",
        "text": "Seq2Seq עם Beam בלבד"
      },
      {
        "id": "a4",
        "text": "Masked Language Model ו‑Next Sentence Prediction"
      },
      {
        "id": "a1",
        "text": "Causal LM ו‑Image Captioning"
      },
      {
        "id": "a2",
        "text": "Autoencoding של תמונות ושמע"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 7 — NN-NLP 11-05-2025",
      "time": "01:16:50–01:31:58",
      "quote": "MLM עם ~15% מסיכה ו‑NSP לקשר בין משפטים עוקבים."
    }
  },
  {
    "id": 51,
    "question": "מה מייחד את GPT‑2 לפי המאמר 'Unsupervised Multitaskers'?",
    "options": [
      {
        "id": "a4",
        "text": "אומן רק על חיזוי המילה הבאה אך מפגין יכולות מרובות משימות ב‑Zero‑Shot"
      },
      {
        "id": "a1",
        "text": "אומן רק על תרגום מקבילי עם תשובות קשיחות"
      },
      {
        "id": "a2",
        "text": "מחליף את Self‑Attention ב‑CNN"
      },
      {
        "id": "a3",
        "text": "משתמש רק ב‑NSP ללא MLM"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 8 — NN-NLP 18-05-2025",
      "time": "00:01:29–00:26:33",
      "quote": "מודל שפה כללי שמגלה יכולות משימה דרך פרומפטים."
    }
  },
  {
    "id": 52,
    "question": "איזו התאמה בין שיטת פענוח לאפיון נכונה?",
    "options": [
      {
        "id": "a1",
        "text": "Top‑k — תמיד דטרמיניסטי; Temperature — מבטל אקראיות"
      },
      {
        "id": "a2",
        "text": "Beam — מקריות גבוהה; Greedy — ליבה רחבה"
      },
      {
        "id": "a3",
        "text": "Top‑p — מגדיל מילון; Beam — מקטין מילון"
      },
      {
        "id": "a4",
        "text": "Beam Search — פחות יצירתי; Top‑p — שליטה בהגרלת הליבה"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 8 — NN-NLP 18-05-2025",
      "time": "00:43:43–01:51:00",
      "quote": "הוסברו Beam, Top‑k, Top‑p (Nucleus) ו‑Temperature."
    }
  },
  {
    "id": 53,
    "question": "מהי למידה 'Few‑Shot/In‑Context' כפי שהודגמה ב‑GPT‑3?",
    "options": [
      {
        "id": "a2",
        "text": "אימון מחדש מאפס על BookCorpus"
      },
      {
        "id": "a3",
        "text": "שימוש רק בטוקן [CLS] לכל משימה"
      },
      {
        "id": "a4",
        "text": "המודל לומד ממש few דוגמאות בתוך הפרומפט ללא עדכון משקולות"
      },
      {
        "id": "a1",
        "text": "Fine‑tuning מלא על דאטה חדש בכל שאלה"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 8 — NN-NLP 18-05-2025",
      "time": "01:03:22–01:14:14",
      "quote": "דוגמאות בהקשר מאפשרות הסתגלות ללא אימון נוסף."
    }
  },
  {
    "id": 54,
    "question": "למה משמש אובייקט Pipelines ב‑Hugging Face?",
    "options": [
      {
        "id": "a3",
        "text": "עריכת רישיונות באופן ידני"
      },
      {
        "id": "a4",
        "text": "הרצה מהירה של משימות מוגדרות מראש (כמו זירו‑שוט, מסיכה, יצירה)"
      },
      {
        "id": "a1",
        "text": "אחסון קבצים בלבד בענן"
      },
      {
        "id": "a2",
        "text": "בניית דאטה‑סטים גרפיים בלבד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 8 — NN-NLP 18-05-2025",
      "time": "01:29:01–01:39:03",
      "quote": "Pipelines מספקים ממשק פשוט להפעלת מודלים נפוצים."
    }
  },
  {
    "id": 55,
    "question": "מהם שלושת שלבי ה‑RLHF לפי השיעור?",
    "options": [
      {
        "id": "a4",
        "text": "SFT מונחה → מודל תגמול (RM) → אופטימיזציה עם PPO"
      },
      {
        "id": "a1",
        "text": "Pre‑training בלבד → Beam Search → Decoding"
      },
      {
        "id": "a2",
        "text": "Masking → Tokenization → Sampling"
      },
      {
        "id": "a3",
        "text": "NSP → MLM → BPE"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 9 — NN-NLP 08-06-2025",
      "time": "00:04:02–00:10:07",
      "quote": "InstructGPT אימן SFT, בנה RM, ואז אופטם PPO עם קנס KL."
    }
  },
  {
    "id": 56,
    "question": "כיצד מאמנים מודל תגמול (RM) ב‑RLHF?",
    "options": [
      {
        "id": "a1",
        "text": "על חיזוי המילה הבאה בלבד"
      },
      {
        "id": "a2",
        "text": "על יצירת תמונות סינתטיות"
      },
      {
        "id": "a3",
        "text": "על התאמה לקוד מקור בלבד"
      },
      {
        "id": "a4",
        "text": "על דירוגי העדפה בין זוג תשובות עם לוס לוגיסטי (W מול L)"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 9 — NN-NLP 08-06-2025",
      "time": "00:29:59–00:36:51",
      "quote": "לוס משווה סקור של תשובה מועדפת מול פחות מועדפת באמצעות סיגמואיד."
    }
  },
  {
    "id": 57,
    "question": "מה התפקיד של KL‑Divergence באובייקטיב של PPO למודלי שפה?",
    "options": [
      {
        "id": "a2",
        "text": "לבטל את הצורך בתגמול אנושי"
      },
      {
        "id": "a3",
        "text": "להחליף את פונקציית הערך"
      },
      {
        "id": "a4",
        "text": "להעניש סטיות גדולות ממודל ה‑SFT כדי למנוע 'התחכמות'"
      },
      {
        "id": "a1",
        "text": "להגדיל אוטומטית את קצב הלמידה"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 9 — NN-NLP 08-06-2025",
      "time": "00:41:19–00:49:56",
      "quote": "ה‑KL מייצב את המדיניות ושומר קרבה להתפלגות המקור."
    }
  },
  {
    "id": 58,
    "question": "מה מחשבת פונקציית ה‑Advantage ב‑PPO בהקשר של שפה?",
    "options": [
      {
        "id": "a3",
        "text": "רמת הדחיסה של ה‑BPE"
      },
      {
        "id": "a4",
        "text": "עד כמה פעולה (טוקן) טובה מעבר לערך הצפוי במצב הנוכחי"
      },
      {
        "id": "a1",
        "text": "זמן ריצה של המודל בכל שכבה"
      },
      {
        "id": "a2",
        "text": "מספר הטוקנים במילון בלבד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 9 — NN-NLP 08-06-2025",
      "time": "01:32:33–01:40:15",
      "quote": "Advantage משלב תגמולים וקנס KL אל מול מדיניות בסיס."
    }
  },
  {
    "id": 59,
    "question": "מהי המיפוי ל‑RL עבור מודלי צ'אט כפי שתואר?",
    "options": [
      {
        "id": "a4",
        "text": "State = היסטוריית השיחה; Action = טוקן; Reward = סקור RM"
      },
      {
        "id": "a1",
        "text": "State = מילון; Action = Beam; Reward = זמן"
      },
      {
        "id": "a2",
        "text": "State = תמונה; Action = קונבולוציה; Reward = Loss"
      },
      {
        "id": "a3",
        "text": "State = טוקן יחיד; Action = באצ'ינג; Reward = KL בלבד"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 9 — NN-NLP 08-06-2025",
      "time": "01:25:21–01:29:53",
      "quote": "הוגדר מיפוי מרכיבי RL לשיח טקסטואלי."
    }
  },
  {
    "id": 60,
    "question": "מהו המתח בין Exploration ל‑Exploitation ב‑RL?",
    "options": [
      {
        "id": "a1",
        "text": "בחירה אקראית קבועה ללא תגמול"
      },
      {
        "id": "a2",
        "text": "מדידה של דיוק סינטקטי בלבד"
      },
      {
        "id": "a3",
        "text": "ביטול מוחלט של תגמולים שליליים"
      },
      {
        "id": "a4",
        "text": "איזון בין חקירת פעולות חדשות לניצול מה שמביא תגמול גבוה"
      }
    ],
    "correctOptionId": "a4",
    "reference": {
      "videoName": "VIDEO 9 — NN-NLP 08-06-2025",
      "time": "01:20:10–01:22:31",
      "quote": "הוסברו מושגי חקירה וניצול ודוגמאות ממשחקים."
    }
  }
]